## 렉서

렉싱: 소스코드를 입력으로 받고, 소스코드를 표현하는 토큰 열을 결과로 출력

- 렉서는 소스코드를 훑어가면서 토큰을 인식할 때마다 결과를 출력
- 렉서 자체는 토큰을 저장하기 위한 별도의 버퍼 불필요

### 동작 방식

1. 렉서를 `초기화`할 때 소스코드를 인자로 전달.
2. 초기화 이후에는 반복적으로 다음 토큰을 출력하는 `NextToken` 메서드를 호출하며 토큰/문자 단위로 소스코드를 훑음.

cf) 상용 환경에서는 파일명과 행 번호를 토큰에 추가하여 렉싱 및 파싱 과정에서의 에러를 더 쉽게 추적할 수 있도록 함. 렉서를 초기화할 때 파일명과 `io.Reader`로 초기화.

### Lexer 초기화 구현

```go
package lexer

import "monkey/token"

// position과 readPosition는 1만큼 차이나는 index 값
// ch은 현재 position의 문자
type Lexer struct {
	input        string
	position     int  // current position in input (points to current char)
	readPosition int  // current reading position in input (after current char)
	ch           byte // current char under examination
}

func New(input string) *Lexer {
	l := &Lexer{input: input}
	l.readChar() // index=0에 해당되는 문자 읽은 상태로 초기화 완료
	return l
}

func (l *Lexer) readChar() {
	// ch 재할당: 현재 읽는 문자, 즉 readChar 실행 후 position 값에 해당되는 위치의 문자로 재할당.
	if l.readPosition >= len(l.input) {
		l.ch = 0 // EOF 혹은 아직 아무 것도 읽지 않은 상태
	} else {
		l.ch = l.input[l.readPosition]
	}
	l.position = l.readPosition // position와 readPosition는 무조건 1만큼 차이.
	l.readPosition += 1
}
```

- 주의: 유니코드는 문자 하나에 복수의 바이트가 할당됨. ch의 경우 byte 타입이므로 ASCII 문자만 지원. 유니코드, UTF-8 지원을 위해서는 rune 타입으로 수정하고 `input`을 읽는 방식 수정 필요.

### Lexer 토큰 변환 구현

현재 검사하고 있는 문자(`ch`)를 보고 그에 대응하는 토큰을 반환.

```go
func (l *Lexer) NextToken() token.Token {
	var tok token.Token

	// l.skipWhitespace()

	switch l.ch {
	case '=':
	    // "=="인지 아닌지 체크
		if l.peekChar() == '=' {
			ch := l.ch
			l.readChar()
			literal := string(ch) + string(l.ch)
			tok = token.Token{Type: token.EQ, Literal: literal}
		} else {
			tok = newToken(token.ASSIGN, l.ch)
		}
	case '+':
		tok = newToken(token.PLUS, l.ch)
	case '-':
		tok = newToken(token.MINUS, l.ch)
	case '!':
	    // "!="인지 아닌지 체크
		if l.peekChar() == '=' {
			ch := l.ch
			l.readChar()
			literal := string(ch) + string(l.ch)
			tok = token.Token{Type: token.NOT_EQ, Literal: literal}
		} else {
			tok = newToken(token.BANG, l.ch)
		}
	case '/':
		tok = newToken(token.SLASH, l.ch)
	case '*':
		tok = newToken(token.ASTERISK, l.ch)
	case '<':
		tok = newToken(token.LT, l.ch)
	case '>':
		tok = newToken(token.GT, l.ch)
	case ';':
		tok = newToken(token.SEMICOLON, l.ch)
	case ',':
		tok = newToken(token.COMMA, l.ch)
	case '{':
		tok = newToken(token.LBRACE, l.ch)
	case '}':
		tok = newToken(token.RBRACE, l.ch)
	case '(':
		tok = newToken(token.LPAREN, l.ch)
	case ')':
		tok = newToken(token.RPAREN, l.ch)
	case 0:
		tok.Literal = ""
		tok.Type = token.EOF
	// default:
	// 	if isLetter(l.ch) {
	// 		tok.Literal = l.readIdentifier()
	// 		tok.Type = token.LookupIdent(tok.Literal)
	// 		return tok
	// 	} else if isDigit(l.ch) {
	// 		tok.Type = token.INT
	// 		tok.Literal = l.readNumber()
	// 		return tok
	// 	} else {
	// 		tok = newToken(token.ILLEGAL, l.ch)
	// 	}
	}

	l.readChar() // 다음 위치로 커서 이동
	return tok // 기존 커서의 문자에 대응되는 토큰 반환
}

// func (l *Lexer) skipWhitespace() {
// 	for l.ch == ' ' || l.ch == '\t' || l.ch == '\n' || l.ch == '\r' {
// 		l.readChar()
// 	}
// }

// 현재 커서의 ch가 아닌 다음 커서의 ch 반환
func (l *Lexer) peekChar() byte {
	if l.readPosition >= len(l.input) {
		return 0
	} else {
		return l.input[l.readPosition]
	}
}

func newToken(tokenType token.TokenType, ch byte) token.Token {
	return token.Token{Type: tokenType, Literal: string(ch)}
}
```

### 식별자, 예약어, 숫자, 공백 대응

```go
func (l *Lexer) NextToken() token.Token {
	var tok token.Token

	// 공백 및 줄바꿈 대응
	l.skipWhitespace()

	switch l.ch {
	// ...
	default:
	    // 현재 문자가 글자로 시작하는 경우, 식별자 혹은 예약어 토큰 반환
		if isLetter(l.ch) {
			tok.Literal = l.readIdentifier() // 연속된 문자들 읽기
			tok.Type = token.LookupIdent(tok.Literal) // 예약어/식별자(변수명)
			return tok
		// 현재 문자가 숫자로 시작하는 경우, INT 타입의 토큰 반환
		} else if isDigit(l.ch) {
			tok.Type = token.INT
			tok.Literal = l.readNumber()
			return tok
		} else {
			// 대응되지 않는 경우
			tok = newToken(token.ILLEGAL, l.ch)
		}
	}
	// ...
}

// 연속되는 문자들 전부 읽기
func (l *Lexer) readIdentifier() string {
	position := l.position
	for isLetter(l.ch) {
		l.readChar()
	}
	return l.input[position:l.position]
}

// 연속되는 숫자들 전부 읽기
func (l *Lexer) readNumber() string {
	position := l.position
	for isDigit(l.ch) {
		l.readChar()
	}
	return l.input[position:l.position]
}

func isLetter(ch byte) bool {
	return 'a' <= ch && ch <= 'z' || 'A' <= ch && ch <= 'Z' || ch == '_'
}

func isDigit(ch byte) bool {
	return '0' <= ch && ch <= '9'
}

// eatWhitespace, consumeWhitespace 등
func (l *Lexer) skipWhitespace() {
	// 언어에 따라 줄바꿈 문자에 대해 별도의 토큰 생성하기도 함
	for l.ch == ' ' || l.ch == '\t' || l.ch == '\n' || l.ch == '\r' {
		l.readChar()
	}
}
```

```go
package token

func LookupIdent(ident string) TokenType {
	if tok, ok := keywords[ident]; ok {
		return tok // 예약어
	}
	return IDENT // 식별자(변수명)
}
```

---

### 테스트

```go
package lexer

import (
	"testing"
	"monkey/token"
)

func TestBasics(t *testing.T) {
	input := `let five = 5;`

	tests := []struct {
		expectedType    token.TokenType
		expectedLiteral string
	}{
		{token.LET, "let"},
		{token.IDENT, "five"},
		{token.ASSIGN, "="},
		{token.INT, "5"},
		{token.SEMICOLON, ";"},
		{token.EOF, ""},
	}

	l := New(input)

	for i, tt := range tests {
		tok := l.NextToken()

		if tok.Type != tt.expectedType {
			t.Fatalf("tests[%d] - tokentype wrong. expected=%q, got=%q",
				i, tt.expectedType, tok.Type)
		}
		if tok.Literal != tt.expectedLiteral {
			t.Fatalf("tests[%d] - literal wrong. expected=%q, got=%q",
				i, tt.expectedLiteral, tok.Literal)
		}
	}
}
```
